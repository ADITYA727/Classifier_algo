{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk                       # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd               # pandas dataframe\n",
    "import re                         # regular expression\n",
    "from nltk.corpus import stopwords  \n",
    "from gensim import parsing        # Help in preprocessing the data, very efficiently\n",
    "import gensim\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading in the training data with Pandas\n",
    "df_train = pd.read_csv(\"data_large_5000.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ALE TeamIndia for the upcoming series against West Indies announced INDvWI\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>tagname</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>amp TeamIndia IndvsBan DayandNightTest PinkBal...</td>\n",
       "      <td>cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BharatArmyAwards Here are the nominees for th...</td>\n",
       "      <td>cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ALE TeamIndia for the upcoming series against...</td>\n",
       "      <td>cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Getting into PinkBallTest mode TeamIndia INDvBAN</td>\n",
       "      <td>cricket</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Umpiring in the first ever pink ball match at...</td>\n",
       "      <td>cricket</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  tagname\n",
       "0  amp TeamIndia IndvsBan DayandNightTest PinkBal...  cricket\n",
       "1   BharatArmyAwards Here are the nominees for th...  cricket\n",
       "2   ALE TeamIndia for the upcoming series against...  cricket\n",
       "3   Getting into PinkBallTest mode TeamIndia INDvBAN  cricket\n",
       "4   Umpiring in the first ever pink ball match at...  cricket"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the first few rows and how the text looks like\n",
    "print (df_train['tweets'][2]) , '\\n'\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Dimensions ,Check Null Value and is sum and data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (35300, 2) \n",
      "\n",
      "Null Value Statistics: \n",
      " \n",
      " tweets     0\n",
      "tagname    0\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "Data Type of All Columns:\n",
      " \n",
      " tweets     object\n",
      "tagname    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "## check the dimensions of the table\n",
    "print (\"Shape:\", df_train.shape, '\\n')\n",
    "\n",
    "## Check if there is any NULL values inside the dataset\n",
    "print (\"Null Value Statistics:\", '\\n \\n', df_train.isnull().sum()) ## Sum will tell the total number of NULL values inside the dataset\n",
    "print ('\\n')\n",
    "\n",
    "## Explore the data types of your dataset\n",
    "print (\"Data Type of All Columns:\" '\\n \\n', df_train.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check tagname (unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cricket' 'mobiles' 'food' 'machine learning' 'hollywood'\n",
      " 'happy birthday' 'football' 'bollywood' 'bigg boss' 'politics']\n"
     ]
    }
   ],
   "source": [
    "## Collect all unique author names from author column\n",
    "tag_names = df_train['tagname'].unique()\n",
    "print (tag_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Give specific id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cricket 0\n",
      "mobiles 1\n",
      "food 2\n",
      "machine learning 3\n",
      "hollywood 4\n",
      "happy birthday 5\n",
      "football 6\n",
      "bollywood 7\n",
      "bigg boss 8\n",
      "politics 9\n"
     ]
    }
   ],
   "source": [
    "tag_name_id = {}\n",
    "assign_id = 0\n",
    "for name in tag_names:\n",
    "    tag_name_id[name] = assign_id\n",
    "    assign_id += 1  ## Get a new id for new author\n",
    "    \n",
    "##  Print the dictionary created\n",
    "for key, values in tag_name_id.items():\n",
    "    print (key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change key to value and value to key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cricket\n",
      "1 mobiles\n",
      "2 food\n",
      "3 machine learning\n",
      "4 hollywood\n",
      "5 happy birthday\n",
      "6 football\n",
      "7 bollywood\n",
      "8 bigg boss\n",
      "9 politics\n"
     ]
    }
   ],
   "source": [
    "id_to_tag_name = {v: k for k, v in tag_name_id.items()}\n",
    "for key, values in id_to_tag_name.items():\n",
    "    print (key, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add new column in csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a new column to pandas dataframe, with the author name mapping\n",
    "def get_author_id(tag_name):\n",
    "    return tag_name_id[tag_name]\n",
    "\n",
    "df_train['tag_id'] = df_train['tagname'].map(get_author_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show data(after adding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>tagname</th>\n",
       "      <th>tag_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>amp TeamIndia IndvsBan DayandNightTest PinkBal...</td>\n",
       "      <td>cricket</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BharatArmyAwards Here are the nominees for th...</td>\n",
       "      <td>cricket</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ALE TeamIndia for the upcoming series against...</td>\n",
       "      <td>cricket</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Getting into PinkBallTest mode TeamIndia INDvBAN</td>\n",
       "      <td>cricket</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Umpiring in the first ever pink ball match at...</td>\n",
       "      <td>cricket</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets  tagname  tag_id\n",
       "0  amp TeamIndia IndvsBan DayandNightTest PinkBal...  cricket       0\n",
       "1   BharatArmyAwards Here are the nominees for th...  cricket       0\n",
       "2   ALE TeamIndia for the upcoming series against...  cricket       0\n",
       "3   Getting into PinkBallTest mode TeamIndia INDvBAN  cricket       0\n",
       "4   Umpiring in the first ever pink ball match at...  cricket       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Function preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformText(text):\n",
    "    \n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    \n",
    "    # Convert text to lower\n",
    "    text = text.lower()\n",
    "    # Removing non ASCII chars    \n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r' ',text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Removing all the stopwords\n",
    "    filtered_words = [word for word in text.split() if word not in stops]\n",
    "    \n",
    "    # Removing all the tokens with lesser than 3 characters\n",
    "    filtered_words = gensim.corpora.textcorpus.remove_short(filtered_words, minsize=3)\n",
    "    \n",
    "    # Preprocessed text after stop words removal\n",
    "    text = \" \".join(filtered_words)\n",
    "    \n",
    "    # Remove the punctuation\n",
    "    text = gensim.parsing.preprocessing.strip_punctuation2(text)\n",
    "    \n",
    "    # Strip all the numerics\n",
    "    text = gensim.parsing.preprocessing.strip_numeric(text)\n",
    "    \n",
    "    # Strip multiple whitespaces\n",
    "    text = gensim.corpora.textcorpus.strip_multiple_whitespaces(text)\n",
    "    \n",
    "    # Stemming\n",
    "    return gensim.parsing.preprocessing.stem_text(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# apply funtion on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['tweets'] = df_train['tweets'].map(transformText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print data after prepossing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp teamindia indvsban dayandnighttest pinkballtest \n",
      "\n",
      "bharatarmyaward nomine men limit perform year hurri vote \n",
      "\n",
      "al teamindia upcom seri west indi announc indvwi\n"
     ]
    }
   ],
   "source": [
    "## Print a couple of rows after the preprocessing of the data is done\n",
    "\n",
    "print (df_train['tweets'][0] , '\\n')\n",
    "print (df_train['tweets'][1] , '\\n')\n",
    "print (df_train['tweets'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data in training data(67%) and test data(33%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Sample Size: 23651   Test Sample Size: 11649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15652    musictv hollywood xma song need right hollywoo...\n",
       "30260    humara munna kisi kam hai kya gonnatellmykid t...\n",
       "34761    sana fam best fandom shilpashind fam full enjo...\n",
       "24499    okai let win tottenham awai cfc chelsea superf...\n",
       "27857    granturco lappel chelsea devant ta contr dcisi...\n",
       "                               ...                        \n",
       "16850    somebodi got stand ovat richard jewel premier ...\n",
       "6265     fall pasta thanksgiv menu delici thanksgiv pas...\n",
       "11284    machin learn realli technolog artificialintell...\n",
       "860                get pinkballtest mode teamindia indvban\n",
       "15795              mean receiv defer wwii would point govt\n",
       "Name: tweets, Length: 23651, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Split the data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train['tweets'], df_train['tag_id'], test_size=0.33, random_state=42)\n",
    "print (\"Training Sample Size:\", len(X_train), ' ', \"Test Sample Size:\" ,len(X_test))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23651, 20591)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the word vocabulary out of the data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "X_train_counts.shape\n",
    "\n",
    "## Count of 'mistak' in corpus (mistake -> mistak after stemming)\n",
    "#print ('mistak appears:', count_vect.vocabulary_.get(u'mistak') , 'in the corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of TF-IDF vector : (23651, 20591)\n"
     ]
    }
   ],
   "source": [
    "## Get the TF-IDF vector representation of the data\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print ('Dimension of TF-IDF vector :' , X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prediction part\n",
    "\n",
    "X_new_counts = count_vect.transform(X_test)\n",
    "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
    "\n",
    "predicted = clf.predict(X_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"check.csv\")\n",
    "x_tt = df_train.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'@tiffanylue i know  i was listenin to bad habit earlier and i started freakin at his part =[' => football\n",
      "'Layin n bed with a headache  ughhhh...waitin on your call...' => hollywood\n",
      "'Funeral ceremony...gloomy friday...' => football\n",
      "'wants to hang out with friends SOON!' => machine learning\n",
      "'@dannycastillo We want to trade with someone who has Houston tickets, but no one will.' => cricket\n",
      "\"Re-pinging @ghostridah14: why didn't you go to prom? BC my bf didn't like my friends\" => football\n",
      "\"I should be sleep, but im not! thinking about an old friend who I want. but he's married now. damn, &amp; he wants me 2! scandalous!\" => machine learning\n",
      "'Hmmm. http://www.djhero.com/ is down' => machine learning\n",
      "'@charviray Charlene my love. I miss you' => hollywood\n",
      "\"@kelcouch I'm sorry  at least it's Friday?\" => bollywood\n",
      "'cant fall asleep' => football\n",
      "'Choked on her retainers' => bollywood\n",
      "'Ugh! I have to beat this stupid song to get to the next  rude!' => bollywood\n",
      "'@BrodyJenner if u watch the hills in london u will realise what tourture it is because were weeks and weeks late  i just watch itonlinelol' => machine learning\n",
      "'Got the news' => happy birthday\n",
      "'The storm is here and the electricity is gone' => bollywood\n",
      "'@annarosekerr agreed' => hollywood\n",
      "\"So sleepy again and it's not even that late. I fail once again.\" => machine learning\n",
      "'@PerezHilton lady gaga tweeted about not being impressed by her video leaking just so you know' => bigg boss\n",
      "'How are YOU convinced that I have always wanted you? What signals did I give off...damn I think I just lost another friend' => hollywood\n",
      "\"@raaaaaaek oh too bad! I hope it gets better. I've been having sleep issues lately too\" => bollywood\n",
      "\"Wondering why I'm awake at 7am,writing a new song,plotting my evil secret plots muahahaha...oh damn it,not secret anymore\" => football\n",
      "'No Topic Maps talks at the Balisage Markup Conference 2009   Program online at http://tr.im/mL6Z (via @bobdc) #topicmaps' => machine learning\n",
      "\"I ate Something I don't know what it is... Why do I keep Telling things about food\" => machine learning\n",
      "\"so tired and i think i'm definitely going to get an ear infection.  going to bed &quot;early&quot; for once.\" => football\n",
      "'On my way home n having 2 deal w underage girls drinking gin on da bus while talking bout keggers......damn i feel old' => football\n",
      "\"@IsaacMascote  i'm sorry people are so rude to you, isaac, they should get some manners and know better than to be so lewd!\" => food\n",
      "'Damm servers still down  i need to hit 80 before all the koxpers pass me' => machine learning\n",
      "\"Fudge.... Just BS'd that whole paper.... So tired.... Ugh I hate school.....  time to sleep!!!!!!!!!!!\" => cricket\n",
      "'I HATE CANCER. I HATE IT I HATE IT I HATE IT.' => football\n",
      "'It is so annoying when she starts typing on her computer in the middle of the night!' => cricket\n",
      "'@cynthia_123 i cant sleep' => cricket\n",
      "'I missed the bl***y bus!!!!!!!!' => food\n",
      "'feels strong contractions but wants to go out.  http://plurk.com/p/wxidk' => machine learning\n",
      "'SoCal!  stoked. or maybe not.. tomorrow' => hollywood\n",
      "'Screw you @davidbrussee! I only have 3 weeks...' => food\n",
      "\"@ether_radio yeah :S i feel all funny cause i haven't slept enough  i woke my mum up cause i was singing she's not impressed :S you?\" => bigg boss\n",
      "'I need skott right now' => hollywood\n",
      "'has work this afternoon' => cricket\n",
      "'@GABBYiSACTiVE Aw you would not unfollow me would you? Then I would cry' => bollywood\n",
      "\"mmm much better day... so far! it's still quite early. last day of #uds\" => machine learning\n",
      "'@DavidArchie &lt;3 your gonna be the first  twitter ;) cause your amazing lol. come to canada  would do anything to see you perform' => hollywood\n",
      "'just picked up her Blackberry from the middle of the street! Both she and it are crushed!' => happy birthday\n",
      "\"Why do I have the feeling I should be packing and hitting for SFO around this time of the year? I think I'm missing something...\" => food\n",
      "'@creyes middle school and elem. High schools will remain open for those who need credits to graduate. Cali is broken' => football\n",
      "\"Bed!!!!!... its time,..... hope i go to school tomorrow, all though i don't feel very well right now\" => hollywood\n",
      "'@onscrn Ahh.  ... Well, I was hoping that I could learn some stuff on the way. ... Why not you and I work on separate things but also' => hollywood\n",
      "\"I'm having a problem with my photo here in twitter amf!!!...can't see my face!\" => football\n",
      "\"@jakeboyd, oh noooo!  if i blow a tire you're reaaaally going to have to send up some batman smoke.\" => food\n",
      "'wnna take a bath!!!!' => machine learning\n",
      "'Chocolate milk is so much better through a straw. I lack said straw' => football\n",
      "'why am i so tired?' => food\n",
      "'@djmicdamn hey yu lil fucker i textd yu' => cricket\n",
      "\"@Mennard time diff and i've just been wrapped up in day to day stuff so i havent been tweeting. talk soon,must sleep...up in 6hrs\" => happy birthday\n",
      "'@benballer  no way! damn that sucks B!  are you ok?' => football\n",
      "'sucks not being able to take days off of work or have the money to take the trip  so sad' => football\n",
      "'bed...sorta. today was good, sara has strep thought Angelina does to; i shared a water with her B4 they told me, i will prob get it to' => cricket\n",
      "\"@ramtops the recession. her hotel are restructuring how the accounts are done. adds a bit more pressure in the short term but we'll cope\" => happy birthday\n",
      "'@lostluna But I got dibs on Sulu...' => bollywood\n",
      "'@maternitytees Aww  Onward and upwards now, yay! Still sad to leave I bet.' => cricket\n",
      "'@itsgabbith at once haha.  poor aby still gets sore!' => bollywood\n",
      "\"diesel yaris... 70mpg  so sad its not available in the US. That'd be awesome.\" => football\n",
      "'I want to buy this great album but unfortunately i dont hav enuff funds  its &quot;long time noisy&quot;' => football\n",
      "'@Pokinatcha  in all honesty...pain   blech.' => cricket\n",
      "\"Ok ... the passengers ... no one is alive ... they're all dead ... you just don't know it til the end ... then you cry ...\" => bollywood\n",
      "'At home alone with not much to do' => happy birthday\n",
      "\"@DavidCookLove ia so much! i haven't really been happy with any of cooks choices for singles.\" => bollywood\n",
      "\"@vincew @stefanyngo  i fell asleep on the beach and didn't put on enough sunscreen  lol\" => cricket\n",
      "'So i think my son might have the flu cause I def. just cleaned up a stanky puke mess  Poor pumkpin' => football\n",
      "'So great to see Oin &amp; Cynthia.  So happy.  Dinner was great, cute little place.  Too bad Oin got sick afterwards.' => cricket\n",
      "'I cant give @jertronic any bday nudges.' => football\n",
      "\"...and all woman who transfer their first impressions (sexual/maternal) onto a less 'threatening' man -- are themselves as weak as 'Him'\" => hollywood\n",
      "\"Brothers Bloom won't be opening this weekend in El Paso.  I'll just buy Brick and enjoy that until I can watch Brothers Bloom.\" => cricket\n",
      "'says I miss plurking.  http://plurk.com/p/wxion' => hollywood\n",
      "'Bitten to blood by my cat, on my way for a rabies bacterin. Seems 7 shots for 2 months. Never wash my cats at home again, they hate water' => hollywood\n",
      "'I miss Voobys!' => machine learning\n",
      "'@Dancing_Monk Neither are ELP!!' => cricket\n",
      "'@havingmysay  dude, that is my favorite sandwich place ever. ummm did you take PICTURES?' => hollywood\n",
      "\"is sad that shin ae got married...and it wasn't to alex\" => food\n",
      "\"@shondarhimes Sure you will tweet about this when you're back, but news is abuzz about TR Knight's leaving &quot;confirmed&quot; today.  Muy triste.\" => machine learning\n",
      "'@RachelLock22 ohh thursday i have exams.. all day  what about wednesday ?' => bollywood\n",
      "'there was a mix up with my dentist appt this afternoon. so they rescheduled me for tomorrow @ 9am.' => football\n",
      "'@gcrush @nopantsdance i was just thinking about how excited i am for you guys to move, but then i realized how sad i am to see you go.' => hollywood\n",
      "'goooood mooorning people... sun is out.. definitly spring now, we had our first spring hail storm, my car has dimples now..' => happy birthday\n",
      "\"@artfuldodga I love those 'it'sakey' USB sticks. We only have the 4GB in Australia\" => machine learning\n",
      "'fresh prince and sleepy sleeps my nightly routine  gotta go to Dmv early tmrw' => hollywood\n",
      "\"dammit! hulu desktop has totally screwed up my ability to talk to a particular port on one of our dev servers. so i can't watch and code\" => machine learning\n",
      "'@emmarler i am jealous of your mom talking to @taylorswift13. i want to see you all our twittering is making me miss you' => bollywood\n",
      "\"I can't sleep...I keep thinking about the puppy I played with today\" => cricket\n",
      "\".. I'm suppposed to be sleep. But i got some much to do. &amp; i got that one part of the song stuck in my head &quot;your a jerk (iknow)&quot;  blaahh\" => machine learning\n",
      "\"@lepetitagneau what's going on sweetheart?\" => happy birthday\n",
      "\"How can it be so freaking difficult to get a system-wide spellchecker? Shit, I'd settle for an office suite one. Stupid unhelpful Windows\" => hollywood\n",
      "'Last one month due to summer, strawberry is not availble in the Chennai markets!' => happy birthday\n",
      "'@willxxmobb work at 6am. Gotta go to bed soon' => food\n",
      "'@RobertF3 correct! I ADORE him. I just plucked him up and put him under my arm cuz he was cryin.  All better now! Hahaha' => football\n",
      "'@sweeetnspicy hiii im on my ipod...i cant fall asleep' => food\n",
      "'dont wanna work 11-830 tomorrow  but i get paid' => machine learning\n",
      "'feels sad coz i wasnt able to play with the guys!!!  http://plurk.com/p/wxiux' => machine learning\n",
      "'PrinceCharming' => hollywood\n",
      "\"@ cayogial i wanted to come to BZ this summer :/ not so sure anymore... a teacher's life in the summer SUCKS\" => mobiles\n",
      "'First ever dropped call on my mobile. On a call to @Telstra no less! ( being charged for data even though I have a data pack  )' => football\n",
      "'@mrgenius23 You win ... SIGH Rakeem' => bollywood\n",
      "'Oh is that time for real?' => machine learning\n",
      "\"Darn these allergies! I don't like this time of year because of this! I never used to have this problem either\" => bollywood\n",
      "'Oh no one minute too late! Oh well' => cricket\n",
      "'@soviet_star Damn, that sucks' => happy birthday\n",
      "\"@cayogial i wanted to come to BZ this summer :/ not so sure anymore... a teacher's life in the summer SUCKS\" => food\n",
      "'@mileycyrus THIS WEBSITE GAVE ME A VIRUS! When i opened it more windows kept POPPING up' => bollywood\n",
      "'ahh! big scary bug flying around my room!!!!!' => cricket\n",
      "'I wish she knew what she puts me through..She stole my heart, never gave it back..and occasionally she likes to be like look what I have!' => food\n",
      "'is up with a nasty cough i cant be sick i have a huge weekend ahead of me' => football\n",
      "'@justamedicine  That was stone cold   Crazy....  ?' => football\n",
      "\"i'm so tired\" => bollywood\n",
      "'shift time  bbye biochem waaaaahhhhhh!! http://plurk.com/p/wxizo' => food\n",
      "\"@melbournegirl I'm sure some1 will cum out and play. I'm workin through til midnight.\" => cricket\n",
      "'@thecreativeone I second that. I wish it rained more where I am' => hollywood\n",
      "\"@neesabear early happy day of birth in case I don't make it! Very tired from therapy today n just taking my medicine!  misshu! Love ya!\" => cricket\n",
      "'damn it were is Eric or anyone else when you need your hair to be played with' => food\n",
      "\"I'm feel deflated. Ugh. No more dog.\" => bollywood\n",
      "'Allergies suck ducks nuts.     &lt;=====8@8=====&gt;' => cricket\n",
      "'Well it almost was a good day... Guess I just retry tomorrow' => cricket\n",
      "'@IamYeTe Waraku is tasteless and expensive! Portion is so little!!!!!  re: waraku' => machine learning\n",
      "\"@freepbx sounds good. Appreciate the suggestion. Been a week now and we're still offline  Time to ask for a refund...\" => food\n",
      "'@poinktoinkdoink He died.  Wait, what about Magic Jack? I just read it.' => cricket\n",
      "'@britblackbird  youstinkatrespondingtotexts!' => food\n",
      "'wonders why her Karma points turned into 0.00.  http://plurk.com/p/wxj54' => machine learning\n",
      "'Need to pack for CALI CALI! Cannot waittt! Thinking a glass of wine is in order to celebrate my weekend vaca. Still work 2morrow, tho.' => machine learning\n",
      "'Is miserable  i feel like im gona cry  sux!' => happy birthday\n",
      "'@megturney well I ran out of beer so I left. Not sure about the ETA. Waiting waiting waiting. Bleh  Gonna be a long nite methinks.' => football\n",
      "\"$#@! My nose stud fell out and I can't find it  Looks like I'll have to head into Amsterdam today and get a new one\" => bigg boss\n",
      "'claire @breakfastnt love the show, got into the office @ 5am and no radio' => happy birthday\n",
      "'Pats in philly at 2 am. I love it. Mmm cheesesteak.  Miss my boyfriend   but I love vacation.' => food\n",
      "\"*sigh* I'm going to bed... I just don't feel right anymore...\" => football\n",
      "\"What? I focused on Tom so much I didn't see my beloved Barack! Oh no! I must vote for both! Poor president only has 626 votes.\" => cricket\n",
      "'@jwillock EpiCentre Wheelock Place - tourists only..' => hollywood\n",
      "'Now I am depressed after watching so you think you can dance' => food\n",
      "\"Just cross 'cause I'm stuck twiddling my thumbs now, ugh\" => bollywood\n",
      "\"miss 16'th\" => bollywood\n",
      "'@TheHarvardian I know. But like I said, I have no idea how long it takes for them to investigate this stuff.' => machine learning\n",
      "'I cant sleep, but im too sore to move' => happy birthday\n",
      "'@LaFloozita http://twitpic.com/4phze - Awe! I miss my baby' => football\n",
      "'@softtouchme just answered you- never learned how to write in French- just basic stuff-' => food\n",
      "'what is it with chocolates? i just can never say no' => bigg boss\n",
      "'Achieving a new appreciation on how a xml build script can really be painful and cumbersome' => football\n",
      "\"@ether_radio i'm too awake now  ill have a nap this afternoon\" => bollywood\n",
      "'@omfgiselle i cant do anything' => food\n",
      "'still needs another 6 hours of sleep' => bollywood\n",
      "\"I'm at work\" => cricket\n",
      "'RIP leonardo. You were a great mini fiddler crab' => bollywood\n",
      "'Morning tweeple,way to early again' => food\n",
      "'Last day working for the Uni today, sad times' => cricket\n",
      "\"@NisforNeemah thanks neemah. I'm gonna be soooo close to you and izzy, yet so far\" => food\n",
      "'My head hurts so bad I could scream!' => happy birthday\n",
      "'i just go up and IM SO TIRED and my airmatras is broken somewhere and now im in even more pain' => bollywood\n",
      "'New work wellness challenge not going well.  I committed to not check email between 10 pm and 6 am.  Failed on first day.  Twice' => hollywood\n",
      "'New blog post: [Blog] auto insuran...: I just found that my auto insurance policy had been expired. ( I am too careless  ...) .. Meanw ...' => happy birthday\n",
      "'where are all ma bestfriends at ? , MIA or wat ?' => bollywood\n",
      "\"oh men!!!!!!......I really can't see my face........c'mon guys!!!\" => bollywood\n",
      "\"I'm showing my age. Renewed my tags last week, went down $20. Got my insurance renewal email today, went down $100. I don't wanna be old\" => hollywood\n",
      "'my sole supporter is not my sole supporter' => machine learning\n",
      "\"don't you hate it when you finish all your work and there's still 1.25 hours left of work time\" => food\n",
      "'@Emilyyy16 urgh stop it guys' => bollywood\n",
      "'is still missing her husband.    I really want him home.' => hollywood\n",
      "'I miss my puppy' => football\n",
      "'2 days of this month left, and I only have 400MB left on my onpeak downloads.' => bollywood\n",
      "\"@IdleThumbs Up is out?  I didn't get the memo   It looks amazing.\" => cricket\n",
      "\"@nzdeany I've given up on pizza - kids would never let me have my fav  (the hot ones)\" => happy birthday\n",
      "\"Up reading tabloids about other people's lives...thinking what I'm gonna do with mine when my baby leaves?\" => football\n",
      "'@BarbSchaefer yearling in pet home died... very sad for their whole family' => football\n",
      "\"fun in the sun hmmm hell no it's cold\" => happy birthday\n",
      "'I got a giant splinter stuck up underneath my finger nail today at Muppets...it hurt.  But I got to go to First Aid for the first time!' => cricket\n",
      "\"@heresmyhello92 We hate change, so of course. We're fans for life. &lt;3 Oh and if we haven't met Rob by then something went horribly wrong.\" => football\n",
      "'is feeling sad... for some reason..  http://plurk.com/p/wxji3' => machine learning\n",
      "\"well fuck- this new pain med has an odd warning that actually applies to me. i can't take this. -chan is displeased.\" => football\n",
      "'just uploaded my new blog... a painful story about an 80s year old man who cried because he wanted to die... very sad' => football\n",
      "\"@juneyee i don't think so.  I WANT DETAILS.\" => machine learning\n",
      "'@jackgraycnn Hi...!!! Who is Mary Poppins???' => machine learning\n",
      "'@jaychuck Its so addicting, but its kind of a curse to do them at night time. Everytime i do one, i feel like making music afterwards' => bigg boss\n",
      "\"my last tweet didn't send  bad phone\" => cricket\n",
      "\"@relly1  OMG Ur alive!!! LOL  2day has gone sooo slow  I'm going insane Grrr You doing anything tonight?\" => hollywood\n",
      "\"Have a headache  I'm going to bed. Goodnight!\" => bigg boss\n",
      "'@taxidermi  I was watching Parental Control' => bollywood\n",
      "'@JessiJaeJoplin did you get them from california vintage? ahahah they have the BEST dresses, i want them but i dont have ebay' => happy birthday\n",
      "'I just saw pics from this past Thanksgiving and am sad because Grandma was in them.' => bollywood\n",
      "'@jertronic it wont let me' => bollywood\n",
      "\"Took a shift tomorrow.    I don't really feel like working right now.\" => food\n",
      "'but now i have no money for a phone' => bollywood\n",
      "'Spent last night in A&amp;E (ER). Wife tangled wheels with Daughter &amp; hit the deck. Dislocation &amp; fracture resulted.' => bollywood\n",
      "\"@TheLastDoctor 9 days  I'm about ready to visit Torchwood and see if they've heard anything\" => happy birthday\n",
      "\"@Bern_morley where are you? In Bris? I can't hear any thunder\" => hollywood\n",
      "'bec vs fat food   --- winner = fat food  but not this weeknend, ill beat it!' => food\n",
      "'Too bad the Red Devils.. disappointing to say the least' => happy birthday\n",
      "'I had a dream about a pretty pretty beach and there was no beach when I woke up' => cricket\n",
      "'@xdjio Have a 3ware 9650SE, not fast enough for 3x X25-M SSD RAID5. 800Mhz IOP on the HPT. Might try an Adaptec 5405 (1.2Ghz).' => football\n",
      "'when da heck will the garage man get here I ask you.. WHEN..' => cricket\n",
      "\"Ate Mandy, please forgive me. I really am sorry. I don't wanna lose my Bff\" => food\n",
      "\"@melluffsyew Umm yeah. That's probably a pretty good note to self because eeeeeewwwwwwww.\" => football\n",
      "'why are plane tickets so expensive' => cricket\n",
      "\"(@wendyisastar) @melluffsyew Umm yeah. That's probably a pretty good note to self because eeeeeewwwwwwww.\" => hollywood\n",
      "'Needs a job BADLY!!!' => food\n",
      "'@AlexanderGWhite daaammmnnnnn I do wish I was there.' => food\n"
     ]
    }
   ],
   "source": [
    "counter  = 0\n",
    "for doc, category in zip(x_tt, predicted):\n",
    "    print('%r => %s' % (doc, id_to_tag_name[category]))\n",
    "    if(counter == 200):\n",
    "        break\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       Directory  results  Created \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "\n",
    "sys.stdout.write(\"%s\" % (\" \" * toolbar_width))\n",
    "sys.stdout.flush()\n",
    "sys.stdout.write(\"\\b\" * (toolbar_width+1))\n",
    "\n",
    "dirName = 'results'\n",
    " \n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir(dirName)\n",
    "    print(\"Directory \" , dirName ,  \" Created \") \n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , dirName ,  \" already exists\")\n",
    "\n",
    "## predictions for first 10 test samples\n",
    "text_hollywood = ''\n",
    "text_bollywood = ''\n",
    "text_ml = ''\n",
    "text_football = ''\n",
    "text_bigg_boss = ''\n",
    "text_food = '' \n",
    "text_hp_day = ''\n",
    "text_politics = ''\n",
    "text_mobiles = ''\n",
    "text_cricket = ''\n",
    "counter  = 0\n",
    "for doc, category in zip(X_test, predicted):\n",
    "    if id_to_tag_name[category] == \"hollywood\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_hollywood != \"\"):\n",
    "            text_hollywood = text_hollywood + '\\n' + doc\n",
    "        else:\n",
    "            text_hollywood = doc\n",
    "        f.write(text_hollywood)\n",
    "        f.close()\n",
    "    if id_to_tag_name[category] == \"bollywood\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_bollywood != \"\"):\n",
    "            text_bollywood = text_bollywood + '\\n' + doc\n",
    "        else:\n",
    "            text_bollywood = doc\n",
    "        f.write(text_bollywood)\n",
    "        f.close()\n",
    "    if id_to_tag_name[category] == \"machine learning\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_ml != \"\"):\n",
    "            text_ml = text_ml + '\\n' + doc\n",
    "        else:\n",
    "            text_ml = doc\n",
    "        f.write(text_ml)\n",
    "        f.close()\n",
    "    if id_to_tag_name[category] == \"football\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_football != \"\"):\n",
    "            text_football = text_football + '\\n' + doc\n",
    "        else:\n",
    "            text_football = doc\n",
    "        f.write(text_football)\n",
    "        f.close()\n",
    "    if id_to_tag_name[category] == \"bigg boss\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_bigg_boss != \"\"):\n",
    "            text_bigg_boss = text_bigg_boss + '\\n' + doc\n",
    "        else:\n",
    "            text_bigg_boss = doc\n",
    "        f.write(text_bigg_boss)\n",
    "        f.close()\n",
    "    if id_to_tag_name[category] == \"food\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_food != \"\"):\n",
    "            text_food = text_food + '\\n' + doc\n",
    "        else:\n",
    "            text_food = doc\n",
    "        f.write(text_food)\n",
    "        f.close()\n",
    "    if id_to_tag_name[category] == \"happy birthday\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_hp_day != \"\"):\n",
    "            text_hp_day = text_hp_day + '\\n' + doc\n",
    "        else:\n",
    "            text_hp_day = doc\n",
    "        f.write(text_hp_day)\n",
    "        f.close() \n",
    "    if id_to_tag_name[category] == \"politics\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_politics != \"\"):\n",
    "            text_politics = text_politics + '\\n' + doc\n",
    "        else:\n",
    "            text_politics = doc\n",
    "        f.write(text_politics)\n",
    "        f.close() \n",
    "    if id_to_tag_name[category] == \"mobiles\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_mobiles != \"\"):\n",
    "            text_mobiles = text_mobiles + '\\n' + doc\n",
    "        else:\n",
    "            text_mobiles = doc\n",
    "        f.write(text_mobiles)\n",
    "        f.close()\n",
    "    if id_to_tag_name[category] == \"cricket\":\n",
    "        f= open('results/'+id_to_tag_name[category]+\".txt\",\"w+\")\n",
    "        if (text_cricket != \"\"):\n",
    "            text_cricket = text_cricket + '\\n' + doc\n",
    "        else:\n",
    "            text_cricket = doc\n",
    "        f.write(text_cricket)\n",
    "        f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9459181045583311"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predicted == y_test) ## 80% sounds good only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[---------------------------------------->Done]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import sys\n",
    "\n",
    "toolbar_width = 40\n",
    "\n",
    "# setup toolbar\n",
    "sys.stdout.write(\"[%s]\" % (\"\" * toolbar_width))\n",
    "sys.stdout.flush()\n",
    "sys.stdout.write(\"\\b\" * (toolbar_width+1)) # return to start of line, after '['\n",
    "\n",
    "for i in range(toolbar_width):\n",
    "    time.sleep(0.1) # do real work here\n",
    "    # update the bar\n",
    "    sys.stdout.write(\"-\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "sys.stdout.write(\">Done]\\n\") # this ends the progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
